{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from src.agents import create_agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_config = {\"configurable\": {\"thread_id\": \"deep-research-thread-11thronee1\"}}\n",
    "\n",
    "query = \"\"\"\n",
    "Do a deep research on agentech in oklahoma city. you should use the web. Agentech is a new startup in the Insurance Claims space. You need to find out the following:\n",
    "this is their url: https://www.agentech.com/\n",
    "1. Who are the founders of the company?\n",
    "2. What is the funding status of the company?\n",
    "3. What are the recent news articles about the company?\n",
    "4. What are the competitors of the company?\n",
    "5. What are the recent job postings for the company?\n",
    "\"\"\"\n",
    "\n",
    "initial_input = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "result = agent.invoke(initial_input, config=thread_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bfe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e01630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final output structure might vary, adjust this key if needed.\n",
    "if result.get('messages') and isinstance(result['messages'], list) and len(result['messages']) > 1:\n",
    "        final_report_content = result['messages'][-1].content\n",
    "        # Assuming the report is in the content of the last message\n",
    "        print(\"\\n--- FINAL REPORT ---\")\n",
    "        print(final_report_content)\n",
    "else:\n",
    "        print(\"Could not find final report in the result.\")\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb57d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c196e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.agent import deep_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b16a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\pault\\AppData\\Local\\Temp\\ipykernel_28880\\3955347913.py\", line 5, in <module>\n",
      "    from src.agents.agent import deep_agent\n",
      "  File \"c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\DeepResearchAgent\\src\\agents\\__init__.py\", line 16, in <module>\n",
      "    from .agent import create_agent, deep_agent\n",
      "  File \"c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\DeepResearchAgent\\src\\agents\\agent.py\", line 5, in <module>\n",
      "    from src.agents.agents import create_agent as create_react_agent\n",
      "  File \"c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\DeepResearchAgent\\src\\agents\\agents\\__init__.py\", line 5, in __getattr__\n",
      "    from src.agents.agents.react_agent import create_agent\n",
      "  File \"c:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1c. App\\Projects\\DeepResearchAgent\\src\\agents\\agents\\react_agent.py\", line 19, in <module>\n",
      "    from langchain_core.language_models import (\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\langchain_core\\language_models\\__init__.py\", line 99, in __getattr__\n",
      "    result = import_attr(attr_name, module_name, __spec__.parent)\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\langchain_core\\_import_utils.py\", line 35, in import_attr\n",
      "    module = import_module(f\".{module_name}\", package=package)\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 31, in <module>\n",
      "    from langchain_core.language_models.base import (\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\langchain_core\\language_models\\base.py\", line 41, in <module>\n",
      "    from transformers import GPT2TokenizerFast  # type: ignore[import-not-found]\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\transformers\\__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\transformers\\utils\\__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 51, in <module>\n",
      "    import torch  # noqa: F401\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\pault\\anaconda3\\envs\\app_project\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "from tavily import TavilyClient\n",
    "from src.agents.agent import deep_agent\n",
    "from typing import Any, Callable, List, Optional, cast, Dict, Literal, Union\n",
    "import os \n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "# It's best practice to initialize the client once and reuse it.\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "# # Search tool to use to do research\n",
    "# def internet_search(\n",
    "#     query: str,\n",
    "#     max_results: int = 5,\n",
    "#     topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
    "#     include_raw_content: bool = False,\n",
    "# ):\n",
    "#     \"\"\"Run a web search\"\"\"\n",
    "#     search_docs = tavily_client.search(\n",
    "#         query,\n",
    "#         max_results=max_results,\n",
    "#         include_raw_content=include_raw_content,\n",
    "#         topic=topic,\n",
    "#     )\n",
    "#     return search_docs\n",
    "\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.tools.structured import StructuredTool\n",
    "\n",
    "# Load environment variables from a .env file for local development.\n",
    "load_dotenv()\n",
    "\n",
    "# --- Pydantic Input Schema for Robust Validation ---\n",
    "class TavilySearchInput(BaseModel):\n",
    "    \"\"\"Input schema for the Tavily Search tool.\"\"\"\n",
    "    query: str = Field(..., description=\"The search query to look up.\")\n",
    "    max_results: Optional[int] = Field(\n",
    "        default=5, description=\"The maximum number of search results to return.\"\n",
    "    )\n",
    "    search_depth: Optional[Literal[\"basic\", \"advanced\"]] = Field(\n",
    "        default=\"advanced\", description=\"The depth of the search: 'basic' or 'advanced'.\"\n",
    "    )\n",
    "    topic: Optional[Literal[\"general\", \"news\", \"finance\"]] = Field(\n",
    "        default=\"general\", description=\"The topic for the search.\"\n",
    "    )\n",
    "    include_domains: Optional[List[str]] = Field(\n",
    "        default=None, description=\"A list of domains to specifically include in the search.\"\n",
    "    )\n",
    "    exclude_domains: Optional[List[str]] = Field(\n",
    "        default=None, description=\"A list of domains to specifically exclude from the search.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class TavilySearchTool:\n",
    "    \"\"\"\n",
    "    A robust, production-ready tool for performing web searches with Tavily.\n",
    "\n",
    "    This class encapsulates the logic for the search tool, using Pydantic for\n",
    "    input validation and providing a secure way to handle API keys for both\n",
    "    local development and production deployment.\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initializes the tool and securely configures the API key.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.getenv(\"TAVILY_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"Tavily API key not provided. Please pass it to the constructor \"\n",
    "                \"or set the TAVILY_API_KEY environment variable.\"\n",
    "            )\n",
    "        # Instantiate the TavilySearch tool from the correct package once.\n",
    "        self.tool = TavilySearch(tavily_api_key=self.api_key)\n",
    "\n",
    "\n",
    "    def run(self, **kwargs) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Executes the Tavily search with validated input.\n",
    "\n",
    "        This method is designed to be wrapped by a LangChain StructuredTool.\n",
    "        It takes keyword arguments that are validated by the Pydantic schema.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate the input using the Pydantic model\n",
    "            validated_args = TavilySearchInput(**kwargs)\n",
    "\n",
    "            # Convert the Pydantic model to a dictionary for invocation.\n",
    "            # exclude_none=True ensures we don't pass optional args if they weren't provided.\n",
    "            invoke_args = validated_args.model_dump(exclude_none=True)\n",
    "\n",
    "            # Perform the search using the validated arguments\n",
    "            result = self.tool.invoke(invoke_args)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            # Return a structured error message if something goes wrong\n",
    "            return [{\"error\": f\"An error occurred during the search: {e}\"}]\n",
    "\n",
    "# --- Create a default instance and a StructuredTool ---\n",
    "\n",
    "# 1. Instantiate our production-ready class.\n",
    "default_tavily_instance = TavilySearchTool()\n",
    "\n",
    "# 2. Create a StructuredTool from the class method.\n",
    "internet_search = StructuredTool.from_function(\n",
    "    name=\"internet_search\",\n",
    "    func=default_tavily_instance.run,\n",
    "    description=(\n",
    "        \"A search engine optimized for comprehensive, accurate, and trusted results. \"\n",
    "        \"Use this for any general web search, research, or to find current events.\"\n",
    "    ),\n",
    "    args_schema=TavilySearchInput\n",
    ")\n",
    "\n",
    "\n",
    "sub_research_prompt = \"\"\"You are a dedicated researcher. Your job is to conduct research based on the users questions.\n",
    "\n",
    "Conduct thorough research and then reply to the user with a detailed answer to their question\n",
    "\n",
    "only your FINAL answer will be passed on to the user. They will have NO knowledge of anything except your final message, so your final report should be your final message!\"\"\"\n",
    "\n",
    "research_sub_agent = {\n",
    "    \"name\": \"research-agent\",\n",
    "    \"description\": \"Used to research more in depth questions. Only give this researcher one topic at a time. Do not pass multiple sub questions to this researcher. Instead, you should break down a large topic into the necessary components, and then call multiple research agents in parallel, one for each sub question.\",\n",
    "    \"prompt\": sub_research_prompt,\n",
    "    \"tools\": [internet_search],\n",
    "}\n",
    "\n",
    "sub_critique_prompt = \"\"\"You are a dedicated editor. You are being tasked to critique a report.\n",
    "\n",
    "You can find the report at `final_report.md`.\n",
    "\n",
    "You can find the question/topic for this report at `question.txt`.\n",
    "\n",
    "The user may ask for specific areas to critique the report in. Respond to the user with a detailed critique of the report. Things that could be improved.\n",
    "\n",
    "You can use the search tool to search for information, if that will help you critique the report\n",
    "\n",
    "Do not write to the `final_report.md` yourself.\n",
    "\n",
    "Things to check:\n",
    "- Check that each section is appropriately named\n",
    "- Check that the report is written as you would find in an essay or a textbook - it should be text heavy, do not let it just be a list of bullet points!\n",
    "- Check that the report is comprehensive. If any paragraphs or sections are short, or missing important details, point it out.\n",
    "- Check that the article covers key areas of the industry, ensures overall understanding, and does not omit important parts.\n",
    "- Check that the article deeply analyzes causes, impacts, and trends, providing valuable insights\n",
    "- Check that the article closely follows the research topic and directly answers questions\n",
    "- Check that the article has a clear structure, fluent language, and is easy to understand.\n",
    "\"\"\"\n",
    "\n",
    "critique_sub_agent = {\n",
    "    \"name\": \"critique-agent\",\n",
    "    \"description\": \"Used to critique the final report. Give this agent some information about how you want it to critique the report.\",\n",
    "    \"prompt\": sub_critique_prompt,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Prompt prefix to steer the agent to be an expert researcher\n",
    "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.\n",
    "\n",
    "The first thing you should do is to write the original user question to `question.txt` so you have a record of it.\n",
    "\n",
    "Use the research-agent to conduct deep research. It will respond to your questions/topics with a detailed answer.\n",
    "\n",
    "When you think you enough information to write a final report, write it to `final_report.md`\n",
    "\n",
    "You can call the critique-agent to get a critique of the final report. After that (if needed) you can do more research and edit the `final_report.md`\n",
    "You can do this however many times you want until are you satisfied with the result.\n",
    "\n",
    "Only edit the file once at a time (if you call this tool in parallel, there may be conflicts).\n",
    "\n",
    "Here are instructions for writing the final report:\n",
    "\n",
    "<report_instructions>\n",
    "\n",
    "CRITICAL: Make sure the answer is written in the same language as the human messages! If you make a todo plan - you should note in the plan what language the report should be in so you dont forget!\n",
    "Note: the language the report should be in is the language the QUESTION is in, not the language/country that the question is ABOUT.\n",
    "\n",
    "Please create a detailed answer to the overall research brief that:\n",
    "1. Is well-organized with proper headings (# for title, ## for sections, ### for subsections)\n",
    "2. Includes specific facts and insights from the research\n",
    "3. References relevant sources using [Title](URL) format\n",
    "4. Provides a balanced, thorough analysis. Be as comprehensive as possible, and include all information that is relevant to the overall research question. People are using you for deep research and will expect detailed, comprehensive answers.\n",
    "5. Includes a \"Sources\" section at the end with all referenced links\n",
    "\n",
    "You can structure your report in a number of different ways. Here are some examples:\n",
    "\n",
    "To answer a question that asks you to compare two things, you might structure your report like this:\n",
    "1/ intro\n",
    "2/ overview of topic A\n",
    "3/ overview of topic B\n",
    "4/ comparison between A and B\n",
    "5/ conclusion\n",
    "\n",
    "To answer a question that asks you to return a list of things, you might only need a single section which is the entire list.\n",
    "1/ list of things or table of things\n",
    "Or, you could choose to make each item in the list a separate section in the report. When asked for lists, you don't need an introduction or conclusion.\n",
    "1/ item 1\n",
    "2/ item 2\n",
    "3/ item 3\n",
    "\n",
    "To answer a question that asks you to summarize a topic, give a report, or give an overview, you might structure your report like this:\n",
    "1/ overview of topic\n",
    "2/ concept 1\n",
    "3/ concept 2\n",
    "4/ concept 3\n",
    "5/ conclusion\n",
    "\n",
    "If you think you can answer the question with a single section, you can do that too!\n",
    "1/ answer\n",
    "\n",
    "REMEMBER: Section is a VERY fluid and loose concept. You can structure your report however you think is best, including in ways that are not listed above!\n",
    "Make sure that your sections are cohesive, and make sense for the reader.\n",
    "\n",
    "For each section of the report, do the following:\n",
    "- Use simple, clear language\n",
    "- Use ## for section title (Markdown format) for each section of the report\n",
    "- Do NOT ever refer to yourself as the writer of the report. This should be a professional report without any self-referential language. \n",
    "- Do not say what you are doing in the report. Just write the report without any commentary from yourself.\n",
    "- Each section should be as long as necessary to deeply answer the question with the information you have gathered. It is expected that sections will be fairly long and verbose. You are writing a deep research report, and users will expect a thorough answer.\n",
    "- Use bullet points to list out information when appropriate, but by default, write in paragraph form.\n",
    "\n",
    "REMEMBER:\n",
    "The brief and research may be in English, but you need to translate this information to the right language when writing the final answer.\n",
    "Make sure the final answer report is in the SAME language as the human messages in the message history.\n",
    "\n",
    "Format the report in clear markdown with proper structure and include source references where appropriate.\n",
    "\n",
    "<Citation Rules>\n",
    "- Assign each unique URL a single citation number in your text\n",
    "- End with ### Sources that lists each source with corresponding numbers\n",
    "- IMPORTANT: Number sources sequentially without gaps (1,2,3,4...) in the final list regardless of which sources you choose\n",
    "- Each source should be a separate line item in a list, so that in markdown it is rendered as a list.\n",
    "- Example format:\n",
    "  [1] Source Title: URL\n",
    "  [2] Source Title: URL\n",
    "- Citations are extremely important. Make sure to include these, and pay a lot of attention to getting these right. Users will often use these citations to look into more information.\n",
    "</Citation Rules>\n",
    "</report_instructions>\n",
    "\n",
    "You have access to a few tools.\n",
    "\n",
    "## `internet_search`\n",
    "\n",
    "Use this to run an internet search for a given query. You can specify the number of results, the topic, and whether raw content should be included.\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "agent = deep_agent(\n",
    "    agent_name=\"coder\",\n",
    "    agent_type=\"coder\",\n",
    "    tools=[internet_search],\n",
    "    prompt_template=research_instructions,\n",
    "    sub_research_prompt=sub_research_prompt,\n",
    "    sub_critique_prompt=sub_critique_prompt,\n",
    "    sub_query_optimizer_prompt = \"\",\n",
    "    sub_insight_extractor_prompt = \"\",\n",
    "    sub_followup_prompt = \"\",\n",
    "    sub_evidence_auditor_prompt = \"\",\n",
    ").with_config({\"recursion_limit\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ded32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from src.agents.agents.utils.runtime import Runtime\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# thread_config = {\"configurable\": {\"thread_id\": \"deep-research-thread-1011-dvsvsth23\"},\n",
    "#                  \"callbacks\": [CallbackHandler()]}\n",
    "store = InMemoryStore()\n",
    "runtime = Runtime(store=store)\n",
    "\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"deep-research-thread-1011-oabinna\"}\n",
    "                 }\n",
    "\n",
    "# query = \"\"\"\n",
    "# Do a deep research on agentech in oklahoma city. you should use the web. Agentech is a new startup in the Insurance Claims space. You need to find out the following:\n",
    "# this is their url: https://www.agentech.com/\n",
    "# 1. Who are the founders of the company?\n",
    "# 2. What is the funding status of the company?\n",
    "# 3. What are the recent news articles about the company?\n",
    "# 4. What are the competitors of the company?\n",
    "# 5. What are the recent job postings for the company?\n",
    "# \"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "Draft a market brief on European battery-recycling startups, including leading players and regulatory headwinds.\n",
    "\n",
    "European Battery-Recycling Startups Market Brief\n",
    "To draft a comprehensive market brief on European battery-recycling startups, I need to gather detailed information on leading startups, specific regulatory frameworks and their impact, market dynamics, and key challenges and opportunities within the European context. The provided background information offers a starting point for market size and general trends but lacks specific details on European startups and granular regulatory headwinds.\n",
    "\n",
    "Identify and Profile Leading European Battery-Recycling Startups\n",
    "\n",
    "Research and identify prominent European startups focused on lithium-ion battery recycling. For each identified startup, collect detailed information on their business model, technology (e.g., hydrometallurgical, pyrometallurgical, direct recycling), funding rounds, key investors, operational scale, strategic partnerships, and their specific value proposition in the European market.\n",
    "\n",
    "Analyze European Regulatory Landscape and Headwinds for Battery Recycling\n",
    "\n",
    "Gather comprehensive details on current and upcoming European regulations, directives, and policies impacting battery recycling, such as the EU Battery Regulation. Specifically, identify regulatory requirements related to collection targets, recycling efficiency rates, recycled content mandates, extended producer responsibility (EPR), and hazardous waste management. Analyze how these regulations create both opportunities and 'headwinds' or challenges for European battery recycling startups.\n",
    "\n",
    "Investigate Market Dynamics and Investment Trends in European Battery Recycling\n",
    "\n",
    "Collect data on the overall market dynamics within the European battery recycling sector, including market size projections, growth drivers, competitive landscape (beyond just startups, to understand the broader context), and supply chain considerations. Research recent investment trends, venture capital funding, and government grants specifically targeting battery recycling startups in Europe.\n",
    "\n",
    "Examine Technological Innovations and Economic Viability Challenges for Startups\n",
    "\n",
    "Research the latest technological innovations and advancements being developed or employed by European battery recycling startups. Additionally, investigate the economic viability challenges faced by these startups, including capital expenditure requirements, operational costs, profitability, and the ability to scale their technologies and operations efficiently.\n",
    "\n",
    "Identify Key Challenges and Opportunities for European Battery Recycling Startups\n",
    "\n",
    "Gather information on a broad range of challenges and opportunities for European battery recycling startups, including but not limited to, feedstock availability, logistics, material purity requirements, competition from established players, global market influences, and the potential for new revenue streams or business models.\n",
    "\"\"\"\n",
    "\n",
    "initial_input = {\"messages\": [HumanMessage(content=query)]}\n",
    "\n",
    "output = agent.invoke(initial_input, config=thread_config, context=runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf27346",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a33a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['files']['final_report.md'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c965a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2a0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47da87d7",
   "metadata": {},
   "source": [
    "> Visualize the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.graph.builder import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b375594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-------------------------Using Mermaid.Ink--------------------------------\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "#-------------------------Using Mermaid + Pyppeteer--------------------------------\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "# %pip install --quiet pyppeteer\n",
    "# %pip install --quiet nest_asyncio\n",
    "\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            curve_style=CurveStyle.LINEAR,\n",
    "            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n",
    "            wrap_label_n_words=9,\n",
    "            output_file_path=None,\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "            background_color=\"white\",\n",
    "            padding=10,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# display(Image(graph.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bf8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Define the initial input for the workflow\n",
    "initial_input = State(\n",
    "    messages=[HumanMessage(content=\"Develop a 3-month marketing plan for a new AI-powered gardening app.\")],\n",
    "    plan_iterations=0\n",
    ")\n",
    "\n",
    "# Define the configuration for this run\n",
    "# Ensure your environment variables for the LLM are set correctly\n",
    "runnable_config = RunnableConfig(\n",
    "    configurable={\"max_plan_iterations\": 2}\n",
    ")\n",
    "\n",
    "# --- Invoke the workflow ---\n",
    "print(\"🚀 Invoking the agent workflow...\")\n",
    "final_state = app.invoke(initial_input, config=runnable_config)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ Workflow finished. Final State:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Pretty-print the final state for inspection\n",
    "pprint(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fb283",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_state['current_plan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63100775",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state[\"__interrupt__\"][0].id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
